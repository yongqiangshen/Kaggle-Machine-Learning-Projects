{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b678ac6c-8cbd-42c4-aa76-0217cb1a73fc",
        "_uuid": "7ad8db20721d8b449b72818a1af02e31aae04ca4"
      },
      "cell_type": "markdown",
      "source": "# 垃圾邮件分类\nhttps://www.kaggle.com/uciml/sms-spam-collection-dataset"
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "d6b1ab40-34da-4645-98c4-4b61c97895f6",
        "_uuid": "0259bd59e4fad41ffd78aa5a510a83ef07daab7f"
      },
      "cell_type": "markdown",
      "source": "拿到数据首先读入拿到数据"
    },
    {
      "metadata": {
        "_cell_guid": "a89760f4-9075-42be-b4c4-c8c87d175c88",
        "_uuid": "4429a082ddabcb23261daec29ee1ae9e68ba2a2e",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\ndata_dir = \"../input/\"\n\ndf = pd.read_csv(data_dir + '/spam.csv', encoding='latin-1')\nprint ('看看数据长什么样子')\nprint (df.head())\n\n# split into train and test\ndata_train, data_test, labels_train, labels_test = train_test_split(\n    df.v2,\n    df.v1, \n    test_size=0.2, \n    random_state=0)  # 加一句注释\n\nprint ('拆分过后的每个邮件内容')\nprint (data_train[:10])  # 为什么加括号， 为什么没有完整输出结果\nprint ('拆分过后每个邮件是否是垃圾邮件')\nprint (labels_train[:10])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7d1e6ee3-af5b-4d66-98fe-5287e58aeb4e",
        "_uuid": "aa89509f18fd365a3ec9518722de0e98263c965d"
      },
      "cell_type": "markdown",
      "source": "统计总共单词个数"
    },
    {
      "metadata": {
        "_cell_guid": "92f9122c-0719-478a-9f26-214591331e11",
        "_uuid": "0277c5949b97ec351692f42399722d6b17358d14",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\ndef GetVocabulary(data): \n    vocab_set = set([])\n    for document in data:\n        words = document.split()\n        for word in words:\n            vocab_set.add(word) \n    return list(vocab_set)\n\nvocab_list = GetVocabulary(data_train)\nprint ('Number of all the unique words : ' + str(len(vocab_list)))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "afc1b1ce-a1f5-4861-b641-db4a08c70e49",
        "_uuid": "257d26b2ad85f4e7fbd528aabf7bc5ae273658c7"
      },
      "cell_type": "markdown",
      "source": "把文章变成词向量\n"
    },
    {
      "metadata": {
        "_cell_guid": "ce0c514a-1dd9-4d6a-97bb-6275fd0cca42",
        "_uuid": "944c3e38e2b9ab9b07c6eb2811de2061582a7324",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\ndef Document2Vector(vocab_list, data):\n    word_vector = np.zeros(len(vocab_list))\n    words = data.split()\n    for word in words:\n        if word in vocab_list:\n            word_vector[vocab_list.index(word)] += 1\n    return word_vector\n\nprint (data_train[1:2,])\nans = Document2Vector(vocab_list,\"the the the\")\nprint (data_train.values[2])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fb0bce06-8225-4ae0-8b7f-2d55eb7be7db",
        "_uuid": "f8e259bb2ef260294478c4ec7929cdd10b68d84a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_matrix = []\nfor document in data_train.values:\n    word_vector = Document2Vector(vocab_list, document)\n    train_matrix.append(word_vector)\n\nprint (len(train_matrix))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9c45b4b5-09d5-462d-a862-5562f63e6415",
        "_uuid": "74c38f0817ee615141bc007c696ddfb65fc8bf34"
      },
      "cell_type": "markdown",
      "source": "做naive bayes 训练，得到训练集每个词概率"
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "565d5c42-f2f9-459a-a985-185ba5845350",
        "_uuid": "e9d96cc04ba14d79664e9a7c947e97785cf9ad4f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "\ndef NaiveBayes_train(train_matrix,labels_train):\n    num_docs = len(train_matrix)\n    num_words = len(train_matrix[0])\n    \n    spam_vector_count = np.ones(num_words);\n    ham_vector_count = np.ones(num_words)  #计算频数初始化为1\n    spam_total_count = num_words;\n    ham_total_count = num_words                  #即拉普拉斯平滑\n    \n    spam_count = 0\n    ham_count = 0\n    for i in range(num_docs):\n        if i % 500 == 0:\n            print ('Train on the doc id:' + str(i))\n            \n        if labels_train[i] == 'spam':\n            ham_vector_count += train_matrix[i]\n            ham_total_count += sum(train_matrix[i])\n            ham_count += 1\n        else:\n            spam_vector_count += train_matrix[i]\n            spam_total_count += sum(train_matrix[i])\n            spam_count += 1\n    \n    print (ham_count)\n    print (spam_count)\n    \n    p_spam_vector = np.log(ham_vector_count/ham_total_count)#注意\n    p_ham_vector = np.log(spam_vector_count/spam_total_count)#注意\n    return p_spam_vector, np.log(spam_count/num_docs), p_ham_vector, np.log(ham_count/num_docs)\n    #返回各类对应特征的条件概率向量\n    #和各类的先验概率\n    \np_spam_vector, p_spam, p_ham_vector, p_ham = NaiveBayes_train(train_matrix, labels_train.values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "052ce8ad-5971-4eb4-82a9-50b43ebc442c",
        "_uuid": "570fa7b2b2a36cdc5da9b41174eadabb50bb472a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data_test.values.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e4415b37-0855-49bd-8926-afe63c668268",
        "_uuid": "1d737630d3f2df2b3d28355f57cabbf594c2bf23",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\n    \ndef Predict(test_word_vector,p_spam_vector, p_spam, p_ham_vector, p_ham):\n    \n    spam = sum(test_word_vector * p_spam_vector) + p_spam\n    ham = sum(test_word_vector * p_ham_vector) + p_ham\n    if spam > ham:\n        return 'spam'\n    else:\n        return 'ham'\n\npredictions = []\ni = 0\nfor document in data_test.values:\n    if i % 200 == 0:\n        print ('Test on the doc id:' + str(i))\n    i += 1    \n    test_word_vector = Document2Vector(vocab_list, document)\n    ans = Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham)\n    predictions.append(ans)\n\nprint (len(predictions))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "c2931bf8-7b90-4d1c-baee-99d77fea4335",
        "_uuid": "f9762a23132a2475c9ea551885edbb5f82f035ff",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# 检测模型\n\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\n\nprint (accuracy_score(labels_test, predictions))\nprint (classification_report(labels_test, predictions))\nprint (confusion_matrix(labels_test, predictions))\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}